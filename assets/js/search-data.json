{
  
    
        "post0": {
            "title": "2. 이미지 크롤링",
            "content": "# Colab 환경일 시 아래의 명령어 실행 후 런타임 재시작 필요. !pip install --upgrade fastai . Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (1.0.61) Collecting fastai Downloading fastai-2.5.3-py3-none-any.whl (189 kB) |████████████████████████████████| 189 kB 5.4 MB/s Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2) Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (21.1.3) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0) Requirement already satisfied: fastprogress&gt;=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.1) Requirement already satisfied: torch&lt;1.11,&gt;=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.10.0+cu111) Collecting fastcore&lt;1.4,&gt;=1.3.22 Downloading fastcore-1.3.27-py3-none-any.whl (56 kB) |████████████████████████████████| 56 kB 4.7 MB/s Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13) Requirement already satisfied: pillow&gt;6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (21.3) Requirement already satisfied: spacy&lt;4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4) Requirement already satisfied: torchvision&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.11.1+cu111) Collecting fastdownload&lt;2,&gt;=0.0.5 Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress&gt;=0.2.4-&gt;fastai) (1.19.5) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (2.0.6) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (0.8.2) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (4.62.3) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (0.4.1) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.0.6) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (3.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.0.0) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (7.4.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (57.4.0) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.1.3) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.0.5) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai) (4.8.2) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai) (3.6.0) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai) (3.10.0.2) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (3.0.4) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (3.0.6) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (0.11.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;fastai) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;fastai) (2018.9) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;fastai) (3.0.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;fastai) (1.1.0) Installing collected packages: fastcore, fastdownload, fastai Attempting uninstall: fastai Found existing installation: fastai 1.0.61 Uninstalling fastai-1.0.61: Successfully uninstalled fastai-1.0.61 Successfully installed fastai-2.5.3 fastcore-1.3.27 fastdownload-0.0.5 . . from fastai.data.all import * from fastai.vision.all import * . path=Path() . - Path가 뭘까? . path? . Type: PosixPath String form: . File: /usr/lib/python3.7/pathlib.py Docstring: Path subclass for non-Windows systems. . On a POSIX system, instantiating a Path should return this object. . Type이 PosixPath이다. Posix란 IEEE가 정한 API 규격이다. 이 규격을 따르면 Unix 계열 OS 간 호환이 가능하며, Unix에서 지원하는 우리에게 익숙한 명령어와 기능을 사용할 수 있다. | . path . Path(&#39;.&#39;) . dot 하나는 현재 내가 위치해있는 디렉토리를 의미한다. | . 추가적으로 dot 두개(..)는 한 단계 상위의 디렉토리를 의미한다. | . path.ls() . (#2) [Path(&#39;.config&#39;),Path(&#39;sample_data&#39;)] . 현재 내가 위치한 디렉토리 내부를 보여준다. | . (path/&#39;sample_data&#39;).ls() . (#6) [Path(&#39;sample_data/anscombe.json&#39;),Path(&#39;sample_data/README.md&#39;),Path(&#39;sample_data/mnist_test.csv&#39;),Path(&#39;sample_data/california_housing_train.csv&#39;),Path(&#39;sample_data/california_housing_test.csv&#39;),Path(&#39;sample_data/mnist_train_small.csv&#39;)] . sample_data 디렉토리 내부를 보여준다. | . Path() 괄호 안에 원하는 경로를 넣어 식별자로 만들어주면 경로 관리가 원활하다. | . path=Path(&#39;/&#39;) . 슬래시(/) 하나는 최상위 디렉토리를 의미한다. | . path.ls() . (#26) [Path(&#39;/proc&#39;),Path(&#39;/home&#39;),Path(&#39;/run&#39;),Path(&#39;/mnt&#39;),Path(&#39;/media&#39;),Path(&#39;/boot&#39;),Path(&#39;/sys&#39;),Path(&#39;/etc&#39;),Path(&#39;/root&#39;),Path(&#39;/var&#39;)...] . 음.. 최상위 디렉토리에는 저런 아이들이 있군! | . 여기서 헷갈릴 수 있는 부분을 짚고 넘어가야 할 것 같다. | . path=Path() path/&#39;asdf&#39; . Path(&#39;asdf&#39;) . path.ls() . (#2) [Path(&#39;.config&#39;),Path(&#39;sample_data&#39;)] . (path/&#39;asdf&#39;).ls() . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-24-eb861c48b2bb&gt; in &lt;module&gt;() -&gt; 1 (path/&#39;asdf&#39;).ls() /usr/local/lib/python3.7/dist-packages/fastcore/xtras.py in ls(self, n_max, file_type, file_exts) 318 res = (o for o in self.iterdir() if has_extns or o.suffix in extns) 319 if n_max is not None: res = itertools.islice(res, n_max) --&gt; 320 return L(res) 321 322 # Cell /usr/local/lib/python3.7/dist-packages/fastcore/foundation.py in __call__(cls, x, *args, **kwargs) 95 def __call__(cls, x=None, *args, **kwargs): 96 if not args and not kwargs and x is not None and isinstance(x,cls): return x &gt; 97 return super().__call__(x, *args, **kwargs) 98 99 # Cell /usr/local/lib/python3.7/dist-packages/fastcore/foundation.py in __init__(self, items, use_list, match, *rest) 103 def __init__(self, items=None, *rest, use_list=False, match=None): 104 if (use_list is not None) or not is_array(items): --&gt; 105 items = listify(items, *rest, use_list=use_list, match=match) 106 super().__init__(items) 107 /usr/local/lib/python3.7/dist-packages/fastcore/basics.py in listify(o, use_list, match, *rest) 55 elif isinstance(o, list): res = o 56 elif isinstance(o, str) or is_array(o): res = [o] &gt; 57 elif is_iter(o): res = list(o) 58 else: res = [o] 59 if match is not None: /usr/local/lib/python3.7/dist-packages/fastcore/xtras.py in &lt;genexpr&gt;(.0) 316 if file_type: extns += L(k for k,v in mimetypes.types_map.items() if v.startswith(file_type+&#39;/&#39;)) 317 has_extns = len(extns)==0 --&gt; 318 res = (o for o in self.iterdir() if has_extns or o.suffix in extns) 319 if n_max is not None: res = itertools.islice(res, n_max) 320 return L(res) /usr/lib/python3.7/pathlib.py in iterdir(self) 1105 if self._closed: 1106 self._raise_closed() -&gt; 1107 for name in self._accessor.listdir(self): 1108 if name in {&#39;.&#39;, &#39;..&#39;}: 1109 # Yielding a path object for these makes little sense FileNotFoundError: [Errno 2] No such file or directory: &#39;asdf&#39; . 위의 상황을 보면, 현재 디렉토리 내부에는 &#39;.config&#39;와 &#39;sample_data&#39; 두 개의 폴더가 존재한다. 그런데 path/&#39;asdf&#39; 라는 명령어를 실행하니 Path가 만들어졌다. 하지만 이 경로의 내부를 ls()함수로 들여다보려 하니 asdf 디렉토리가 존재하지 않는다며 오류가 발생했다. $ rightarrow$ Path()는 실제로 존재하는 경로만을 나타내주는 것이 아님을 알 수 있다. | . | . (path/&#39;asdf&#39;).mkdir() . mkdir()을 통해 새로운 디렉토리를 만들어줄 수 있다. | . path.ls() . (#3) [Path(&#39;.config&#39;),Path(&#39;asdf&#39;),Path(&#39;sample_data&#39;)] . 성공적으로 생성되었음을 확인할 수 있다. | . (path/&#39;asdf&#39;).mkdir() . FileExistsError Traceback (most recent call last) &lt;ipython-input-32-29f7b5b88f8c&gt; in &lt;module&gt;() -&gt; 1 (path/&#39;asdf&#39;).mkdir() /usr/lib/python3.7/pathlib.py in mkdir(self, mode, parents, exist_ok) 1271 self._raise_closed() 1272 try: -&gt; 1273 self._accessor.mkdir(self, mode) 1274 except FileNotFoundError: 1275 if not parents or self.parent == self: FileExistsError: [Errno 17] File exists: &#39;asdf&#39; . 이미 존재하는 폴더는 다시 생성할 수 없다.(에러 발생) | . (path/&#39;asdf&#39;).mkdir(exist_ok=True) . 위와 같은 옵션을 지정해주면 중복되는 폴더가 없을 경우엔 정상적으로 생성되지만, 중복될 경우 에러 메시지를 띄우지 않고 넘어간다. | . (path/&#39;asdf&#39;).rmdir() . rmdir() 함수를 통해 폴더를 삭제할 수 있다. | .",
            "url": "https://woohyeok-moon.github.io/Blog/big%20data%20analysis/2021/12/25/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D2.html",
            "relUrl": "/big%20data%20analysis/2021/12/25/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D2.html",
            "date": " • Dec 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "1. CNN 맛보기",
            "content": "Fastai&#47484; &#51060;&#50857;&#54620; &#44053;&#50500;&#51648;&#50752; &#44256;&#50577;&#51060; &#48516;&#47448; &#47784;&#45944; &#49324;&#50857;&#54644;&#48372;&#44592; . - 데이터 다운로드 및 전처리 . # Colab 환경일 시 아래의 명령어 실행 후 런타임 재시작 필요. !pip install --upgrade fastai . Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (1.0.61) Collecting fastai Downloading fastai-2.5.3-py3-none-any.whl (189 kB) |████████████████████████████████| 189 kB 5.4 MB/s Requirement already satisfied: torchvision&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.11.1+cu111) Collecting fastdownload&lt;2,&gt;=0.0.5 Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.1) Collecting fastcore&lt;1.4,&gt;=1.3.22 Downloading fastcore-1.3.27-py3-none-any.whl (56 kB) |████████████████████████████████| 56 kB 3.8 MB/s Requirement already satisfied: torch&lt;1.11,&gt;=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.10.0+cu111) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1) Requirement already satisfied: spacy&lt;4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2) Requirement already satisfied: pillow&gt;6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2) Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (21.1.3) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (21.3) Requirement already satisfied: fastprogress&gt;=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress&gt;=0.2.4-&gt;fastai) (1.19.5) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (0.4.1) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.1.3) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (7.4.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (57.4.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (0.8.2) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.0.6) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.0.5) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (2.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (1.0.0) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (3.0.6) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;4-&gt;fastai) (4.62.3) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai) (4.8.2) Requirement already satisfied: typing-extensions&gt;=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai) (3.10.0.2) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai) (3.6.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;fastai) (2021.10.8) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (3.0.6) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (1.3.2) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;fastai) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;fastai) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;fastai) (2018.9) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;fastai) (3.0.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;fastai) (1.1.0) Installing collected packages: fastcore, fastdownload, fastai Attempting uninstall: fastai Found existing installation: fastai 1.0.61 Uninstalling fastai-1.0.61: Successfully uninstalled fastai-1.0.61 Successfully installed fastai-2.5.3 fastcore-1.3.27 fastdownload-0.0.5 . . from fastai.vision.all import * . path=untar_data(URLs.PETS)/&#39;images&#39; . . 100.00% [811712512/811706944 00:18&lt;00:00] path . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;) . files = get_image_files(path) . files[2] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_132.jpg&#39;) . 고양이와 강아지 이미지는 파일명의 첫 글자가 각각 대문자, 소문자로 저장되어 있다. | 파일명의 대소문자 구분을 통해 cat, dog를 구별해주는 함수를 구현한다. | . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . label_func(&#39;asdf&#39;) . &#39;dog&#39; . 여기서 사용하는 데이터는 파일명을 통해 종속변수를 분류해놨기 때문에 from_name_func를 사용하여 ImageDataLoaders 오브젝트로 만들어준다. | 인자로는 이미지가 위치한 경로와 파일명들, 위에 구현했던 함수, 이미지의 크기를 변환해주는 Resize함수를 사용한다. | . dls=ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224)) . - show_batch()를 사용한 이미지 preview . dls.show_batch() . dls.show_batch(max_n=16) . - 모델 생성 . learn = cnn_learner(dls, resnet34, metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . 만약 에러가 난다면 아래 코드를 실행한 뒤 커널을 재시작한다. | . # !conda install -c conda-forge ipywidgets -y # !conda install -c conda-forge nodejs -y . - 학습 및 평가 . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.157301 | 0.015782 | 0.007442 | 01:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.038666 | 0.002587 | 0.000677 | 01:16 | . - 예측 . learn.predict(files[0]) . (&#39;dog&#39;, TensorBase(1), TensorBase([1.6300e-05, 9.9998e-01])) . 개일 확률은 0.9999로 매우 높고, 고양이일 확률은 매우 낮다고 예측하였다. | . - 예측 결과를 임의로 보려면? . learn.show_results() . 대체로 잘 맞추는 것을 확인할 수 있다. | . - 오답분석 . interp = Interpretation.from_learner(learn) . interp.plot_top_losses(9) . plot_top_losses() 함수에 문제가 있어 제대로 출력되지 않는 모습이다. 임시로 코드를 고쳐서 사용해보자. | . def plot_top_losses_fix(interp, k, largest=True, **kwargs): losses,idx = interp.top_losses(k, largest) if not isinstance(interp.inputs, tuple): interp.inputs = (interp.inputs,) if isinstance(interp.inputs[0], Tensor): inps = tuple(o[idx] for o in interp.inputs) else: inps = interp.dl.create_batch(interp.dl.before_batch([tuple(o[i] for o in interp.inputs) for i in idx])) b = inps + tuple(o[idx] for o in (interp.targs if is_listy(interp.targs) else (interp.targs,))) x,y,its = interp.dl._pre_show_batch(b, max_n=k) b_out = inps + tuple(o[idx] for o in (interp.decoded if is_listy(interp.decoded) else (interp.decoded,))) x1,y1,outs = interp.dl._pre_show_batch(b_out, max_n=k) if its is not None: #plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), L(self.preds).itemgot(idx), losses, **kwargs) plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), interp.preds[idx], losses, **kwargs) #TODO: figure out if this is needed #its None means that a batch knows how to show itself as a whole, so we pass x, x1 #else: show_results(x, x1, its, ctxs=ctxs, max_n=max_n, **kwargs) . plot_top_losses_fix(interp, 9) . - 데이터에 포함되어있지 않은 사진 분류 . PILImage.create(&#39;cat1.png&#39;) . learn.predict(PILImage.create(&#39;cat1.png&#39;)) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 3.9922e-09])) . PILImage.create(&#39;cat2.png&#39;) . learn.predict(PILImage.create(&#39;cat2.png&#39;)) . (&#39;cat&#39;, TensorBase(0), TensorBase([9.9991e-01, 9.0290e-05])) . PILImage.create(&#39;dog1.png&#39;) . learn.predict(PILImage.create(&#39;dog1.png&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([1.1261e-06, 1.0000e+00])) . PILImage.create(&#39;dog2.png&#39;) . learn.predict(PILImage.create(&#39;dog2.png&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([6.8834e-07, 1.0000e+00])) . PILImage.create(&#39;dog3.png&#39;) . learn.predict(PILImage.create(&#39;dog3.png&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([8.7128e-04, 9.9913e-01])) . 데이터에 포함되어있지 않은 개와 고양이 사진 또한 잘 판단한다. | .",
            "url": "https://woohyeok-moon.github.io/Blog/big%20data%20analysis/2021/12/24/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D1.html",
            "relUrl": "/big%20data%20analysis/2021/12/24/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D1.html",
            "date": " • Dec 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "13. 확률밀도함수 및 확률분포함수",
            "content": "&#54869;&#47456;&#48128;&#46020;&#54632;&#49688;(probability density function) . - 확률변수의 분포형태를 나타내기 위해 저희는 확률밀도함수라는 것을 사용합니다. 확률변수의 분포를 함수의 꼴로 표현해 식과 그래프로 나타낼 수 있게 되며, 우리가 배워왔던 &#39;수학&#39;을 적용할 수 있게 됩니다. . 일반적으로 이산형 확률변수는 &#39;확률질량함수(probability mass function)&#39;, 연속형 확률변수는 &#39;확률밀도함수(probability density function)&#39;로 표현합니다. 하지만 이 블로그에서는 두 가지 경우 모두 확률밀도함수로 통일하여 표현하겠습니다. | . 확률밀도함수의 조건은 아래와 같습니다. | . 1) 이산형 $$ quad text{모든 실수 }x text{에 대하여, }f(x) geq 0 text{이다.}$$ $$ quad text{확률변수 }X text{가 가질 수 있는 유한 또는 가산무한개의 값 }x_1, x_2, cdots text{에 대하여 } f(x_i)&gt;0 text{이며 } sum limits_{ forall x_i}f(x_i)=1 text{이다.}$$ . 2) 연속형 $$ quad text{모든 실수 }x text{에 대하여, }f(x) geq 0 text{이다.}$$ $$ quad int_{- infty}^{ infty}f(x)dx=1$$ . 위의 조건에서 알 수 있는 것은, 이산형 확률변수와 연속형 확률변수의 차이는 합을 나타내는 방법이 $ sum$과 $ int$로 다를 뿐이라는 것입니다. | . 연속형 확률변수의 예를 하나 들어보겠습니다. 구간 (0, 3)에 정의된 함수 $f(x)= frac{x^2}{9}$은 | . $$(1) f(x) geq 0$$ $$ (2) int_0^3 frac{x^2}{9}dx=1$$ . 위의 두 조건을 모두 만족하므로 확률밀도함수입니다. 변수 $X$의 확률밀도함수가 $f(x)$이면, $0&lt;X&lt;2$일 확률은 아래와 같습니다. . $$ begin{aligned} P(0&lt;X&lt;2) =&amp; int_0^2 frac{x^2}{9} dx =&amp; frac{8}{27} end{aligned}$$ import numpy as np import matplotlib.pyplot as plt from matplotlib import font_manager, rc import matplotlib font_name = font_manager.FontProperties(fname=&quot;c:/Windows/Fonts/malgun.ttf&quot;).get_name() rc(&#39;font&#39;, family=font_name) matplotlib.rcParams[&#39;axes.unicode_minus&#39;] = False x = np.linspace(0, 3, 20) x1 = np.linspace(0, 2, 20) y = np.square(x)/9 y1 = np.square(x1)/9 plt.figure(figsize=(10, 6)) plt.xlabel(&#39;X&#39;, fontsize=14) plt.ylabel(&#39;f(x)&#39;, fontsize=14, rotation=0) plt.xticks([0, 1, 2, 3], fontsize=14) plt.yticks([1], fontsize=14) plt.plot(x, y, c=&#39;dodgerblue&#39;) plt.plot(x1, y1, c=&#39;dodgerblue&#39;) plt.axvline(3, .05, .95, linestyle=&#39;--&#39;, c=&#39;gray&#39;) plt.annotate(&#39; &#39;,xy=(0, 1), xytext=(0, -0.08), arrowprops=dict(facecolor=&#39;black&#39;, width=1, shrink=0, headwidth=8)) plt.annotate(&#39; &#39;,xy=(3.1, 0), xytext=(-0.175, 0), arrowprops=dict(facecolor=&#39;black&#39;, width=1, shrink=0, headwidth=8)) plt.fill_betweenx(y1, x1, 2, color=&#39;dodgerblue&#39;) plt.annotate(&#39;넓이=8/27&#39;, xy=(1.5, .1), xytext=(2.5, .3), fontsize=16, arrowprops=dict(facecolor=&#39;black&#39;, width=2, shrink=1, headwidth=8)) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.tick_params(axis=&#39;x&#39;, color=&#39;white&#39;, pad=-1) plt.tick_params(axis=&#39;y&#39;, color=&#39;white&#39;, pad=-25) plt.tight_layout() plt.show() . . &#54869;&#47456;&#48516;&#54252;&#54632;&#49688;(probability distribution function) . - 확률분포함수 $F(X)$는 &#39;누적분포함수&#39; 또는 &#39;누적확률분포함수&#39;라고도 하며, 주어진 점 $x$ 이하인 모든 값을 가질 확률을 누적한다는 의미를 지닌다. 식으로 표현하면 아래와 같습니다. . $$ F(x)= begin{cases} sum limits_{x_i leq x}&amp; text{, X가 이산형인 경우} int_{- infty}^{ infty} &amp; text{, X가 연속형인 경우} end{cases} $$ 확률분포함수의 조건은 아래와 같습니다. | . $$ lim_{x to - infty} F(x) = 0$$ $$ lim_{x to infty} F(x) = 1$$ $$ lim_{h to 0^{+}} F(x+h) = F(x)$$ $$a&lt;b text{이면} F(a) leq F(b)$$ . 확률변수 $X$가 구간 (a, b] 사이의 값을 가질 확률을 확률분포함수를 이용하면 아래와 같이 표현할 수 있습니다. | . $$P(a&lt;X leq b) = F(b) = F(a)$$ .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/10/21/%EC%88%98%ED%86%B513.html",
            "relUrl": "/statistical%20mathematics/2021/10/21/%EC%88%98%ED%86%B513.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "12. 확률변수의 정의",
            "content": "&#54869;&#47456;&#48320;&#49688;(random variable) . - 확률변수 X는 표본공간 S에 정의된 실함수이다. . 변수란 무엇일까요? 특정 조건에 따라 변할 수 있는 어떠한 수를 변수라고 부릅니다. 이와 같은 논리로 확률변수는 확률에 따라서 변하는 수를 의미합니다. 우리가 실험을 할 때 일반적으로 관심을 갖는 것은, 실험으로부터 얻을 수 있는 어떤 수치적인 양입니다. 이 때 실험결과를 표현하는 이 수치적 양을 확률변수라고 합니다. 이 $X$는 표본공간에서 정의되는 하나의 실함수(정의역과 치역이 모두 실수로 표현되는 함수)가 됩니다. | . 사실 이 확률변수 X는 실험을 하기 전에는 저희가 알 수 없습니다. 대신 그 실험에서 나올 수 있는 값과, 그 값이 나올 가능성을 생각하여 나타냅니다. 이를 확률분포라고 합니다. 확률변수는 항상 상응하는 확률분포를 가지며, 확률분포의 다양한 표현 방법은 추후 알아보겠습니다. | . 동전을 3회 독립적으로 반복하여 던지는 시행에서 앞면이 몇 번 나올지에 관심이 있다고 해봅시다. 이 때의 표본공간 S와 그에 대응하는 확률변수 X(=앞면이 나오는 횟수)는 다음과 같습니다. | . S X . HHH | 3 | . HHT | 2 | . HTH | 2 | . THH | 2 | . HTT | 1 | . THT | 1 | . TTH | 1 | . TTT | 0 | . 위와 같이 값이 하나씩 떨어지는(셀 수 있는) 확률변수(0, 1, 2, $ cdots$)를 이산형(discrete) 확률변수라 하고, 셀 수 없는 실직선상의 어떤 구간으로 표현되는 확률변수를 연속형(continous) 확률변수라 합니다. 예를 들어, 확률변수 X를 &#39;특정 사건이 일어날 때까지 걸리는 시간&#39;으로 정의했다면 연속형 확률변수가 되는 것입니다. | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/10/20/%EC%88%98%ED%86%B512.html",
            "relUrl": "/statistical%20mathematics/2021/10/20/%EC%88%98%ED%86%B512.html",
            "date": " • Oct 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "0. 회귀분석 정리",
            "content": "1. &#54665;&#47148; . 1.1 . $$A+B=B+A$$ $$(A+B)+C=A+(B+C)$$ $$(A+B)^{ prime}=A^{ prime} + B^{ prime}$$ . . 1.2 . 일반적으로 $A cdot B neq B cdot A$ 이다. . . 1.3 . $$(A cdot B)^{ prime} = B^{ prime} cdot A^{ prime}$$ . . 1.4 . 모든 정방행렬에 대하여 $A cdot I = I cdot A = A$가 성립한다. . . 1.5 . $$(A cdot B) cdot C = A cdot (B cdot C)$$ $$C(A+B) = C cdot A + C cdot B$$ . . 1.6 . 행렬 $A$와 $B$가 모두 정방행렬이면 $$|AB| = |BA| = |A||B|$$ . . 1.7 . 행렬 $A$가 정방행렬이면 $$|A| = |A^{ prime}|$$ . . 1.8 . 행렬 $A$가 $n times n$ 정방행렬이면 상수 $k$에 대하여 $$|kA|=k^n|A|$$ . . 1.9 . $$tr(AB)=tr(BA)$$ $$tr(A pm B) = tr(A) pm tr(B)$$ $$tr(ABC)=tr(BCA)=tr(CAB)$$ . . 1.10 . $I$를 $n times n$ 단위행렬이라고 하면 $$tr(I) = n$$ . . 1.11 . $$ textrm{tr}(kA) = k textrm{tr}(A)$$ . . 1.12 . 전치행렬 표시와 그 역행렬 표시는 순서를 바꾸어도 된다. $$(A prime)^{-1} = (A^{-1})^{ prime}$$ . . 1.13 . 행렬 $A$와 $B$가 모두 정방행렬이고, $|A| neq 0$, $|B| neq 0$ 이면 $$(AB)^{-1}=B^{-1}A^{-1}$$ . . 1.14 . $$|A^{-1}|= frac{1}{|A|}$$ . . 1.15 . 상수 $k$가 0이 아니고 행렬 $A$가 역행렬이 존재한다면 $$(kA)^{-1} = frac{1}{k}A^{-1}$$ . . 1.16 . 모든 대칭행렬 $A$에 대하여 . $P^{ prime} AP=D$ ($D$는 대각행렬) . 이 되는 직교행렬 P가 반드시 존재한다. . . 1.17 . 직교행렬 $P$에 대하여 다음 관계식이 성립한다. . $1) ; tr(P prime AP) = tr(A)$ $2) ; |P prime AP| = |A|$ $3) ; |P| = 1 , or , -1$ . . 1.18 . 두 개의 행렬 $A$와 $B$가 모두 멱등행렬이고 $AB=BA$이면 행렬 $AB$도 멱등행렬이다. . . 1.19 . $$r(A^{ prime})=r(A)=r(A^{ prime} A)=r(AA^{ prime})$$ . . 1.20 . $$r(A cdot B) leq r(A) , or , r(B)$$ $$r(A vdots B) leq r(A) + r(B)$$ . . 1.21 . 대칭행렬 $A$의 고유치는 모두 실수(real number)이다. . . 1.22 . $n times n$ 대칭행렬 $A$의 고유치를 $ lambda_1, lambda_2, dots, lambda_n$ 이라고 하면 다음 식이 성립한다. . $$tr(A)= sum_{i=1}^n lambda_i$$ $$tr(A^{ prime} A) = sum_{i=1}^n lambda_i^2$$ $$tr(A^{-1})= sum_{i=1}^n lambda_i^{-1}$$ $$|A| = lambda_1, lambda_2, dots, lambda_n = prod_{i=1}^n lambda_i$$ . . 1.23 . 대칭행렬 $A$의 0이 아닌 고유치의 개수는 $r(A)$와 같다. . . 1.24 . 멱등행렬의 고유치는 0 또는 1이다. 따라서 $$tr(A)=r(A)$$ 대칭행렬 $A$의 어떤 고유치 $ lambda_i$에 대하여 $$(A- lambda I)x = 0, 즉 , Ax= lambda_i x$$ 와 같은 연립방정식을 만족시키는 벡터 $x$를 주어진 $ lambda_i$의 고유벡터라고 하며 $p$개의 고유치에 대하여 $p$개의 고유벡터가 존재한다. . . 1.25 . 대칭행렬 $A$가 양정치행렬이 되기 위한 필요충부조건은 어떤 정칙행렬 $C$가 존재하여 $A=CC^{ prime}$와 같이 되는 것이다. . . 1.26 . $n times m$ 행렬 $A$의 계수가 $m , (m&lt;n)$ 일 때 $A^{ prime} A$는 양정치행렬이고 $AA^{ prime}$은 양반정치행렬이다. . . 1.27 . $n times m ,(m&lt;n)$ 행렬 $A$의 계수가 $k(k&lt;m)$일 때 $A^{ prime} A$와 $AA^{ prime}$은 모두 양반정치행렬이다. . . 1.28 . 양(음)정치행렬 $A$의 고유치는 모두 양(음)수이고 양(음)반정치행렬$B$의 고유치는 0 또는 양(음)수이다. . . 1.29 . 어떤 정칙행렬 $C$에 대하여 만약 행렬 $A$가 양정치이면 $C^{ prime} AC$도 양정치이고 만약 $A$가 양반정치이면 $C^{ prime} AC$도 양반정치이다. . . 1.30 . $(n times n)$ 행렬 $A=(a_{ij})$가 양정치행렬이기 위한 필요충분조건은 다음과 같다. . $$a_{11}&gt;0, , begin{pmatrix} a_{11} &amp; a_{12} a_{21} &amp; a_{22} end{pmatrix} &gt; 0, cdots , , begin{pmatrix} a_{11} &amp; a_{12} &amp; cdots &amp; a_{1n} a_{21} &amp; a_{22} &amp; cdots &amp; a_{2n} vdots &amp; vdots &amp; &amp; vdots a_{n1} &amp; a_{n2} &amp; cdots &amp; a_{nn} end{pmatrix} &gt;0$$ . 2. &#44592;&#52488;&#53685;&#44228; . 2.1 . $X$와 $Y$를 확률변수, $k$를 임의의 상수라고 할 때 다음이 성립한다. . $(1) , E(kX) = kE(X)$ $(2) , E(X+k) = E(X) + k $ $(3) , E(X+Y) = E(X) + E(Y)$ . . 2.2 . $X$를 확률변수, $k$를 상수라고 하면 다음이 성립한다. . $(1) Var(X+k) = Var(X)$ $(2) Var(kX) = k^2 Var(X)$ . . 2.3 . $X$와 $Y$ 서로 독립인 확률변수이면, 다음 식이 성립한다. . $1) , Var(X pm Y) = Var(X) + Var(Y)$ $2) , E(XY) = E(X)E(Y)$ $3) , rho_{XY}=0, , sigma_{XY}=0$ . . 2.4 . 상수벡터 $a^ prime=(a_1, a_2, cdots, a_k)$에 대한 확률번수 $X_1, X_2, cdots, X_k$의 선형결합 $a prime x=a_1X_1+a_2X_2+ cdots + a_kX_k$의 기댓값과 분산은 다음과 같다. . $$E(a^ prime x)=a^ prime E(x) = a^ prime mu$$ $$Var(a^ prime x) = a^ prime Var(x)a = a^ prime sum a = sum_{i=1}^k a_i^2 mu_i^2 + 2 sum_{i=1}^k cdot sum_{j=1, i&gt;i}^k a_i a_j sigma_{ij}$$ . 만약 $X_i$들이 서로 독립이면 $ sigma_{ij}=0$이 된다. 또한 변수 $X_i$들이 두 개의 선형식 $ sigma_{i=1}^k a_i X_i$와 $ sum_{i=1}^k b_i X_i$에서 $X_i$들이 서로 독립이면 다음과 같은 공분산 식이 성립한다. . $$cov( sum_{i=1}^k a_i X_i, sum_{i-1}^k b_i X_i) = sum_{i=1}^k a_i b_i sigma^2$$ . . 2.5 . 만약 확률변수 $X$가 $N( mu, sigma)$인 정규분포를 따르고 임의의 상수 $a, c , (c neq 0)$가 존재하여 $Y=a+cX$라는 관계가 성립하면 $$Y sim N(a+c mu, c^2 sigma^2)$$ 과 같은 분포를 따른다. . . 2.6 . $X_1, X_2, dots, X_n$이 서로 독립이고 정규분포를 따르는 확률변수라면 $X_i$들의 선형결합 $Y= sum_{i=1}^n a_i X_i$ ($a_i$는 상수)은 다음과 같은 정규분포를 한다. . $$Y sim N( sum_{i=1}^n a_i E(X), quad sum_{i=1}^n a_i^2 Var(X_i))$$ . . 2.7 . $N( mu, sigma^2)$의 모집단으로부터 $n$개의 표본을 뽑아 표본분산을 $S_*^2= frac{1}{n} sum_{i=1}^n(X_i = bar X)^2$이라고 하면 $X$와 $S_*^2$은 서로 독립이다. . $$ frac{nS_*^2}{ sigma^2} sim chi^2 (n-1)$$ . $S_*^2$ 대신에 불편분산 $S^2= frac{1}{n-1} sum_{i=1}^n(X_i- bar X)^2$을 이용하면 . $$ frac{(n-1)S^2}{ sigma^2} sim chi^2(n-1)$$ . 이다. . . 2.8 . $X_1, X_2, dots, X_n$이 $N( mu, sigma^2)$ 모집단으로부터의 표본이라고 하면 . $$ frac{ bar X - mu}{ frac{S}{ sqrt n}} sim t(n-1)$$ .",
            "url": "https://woohyeok-moon.github.io/Blog/regression/2021/10/03/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D0.html",
            "relUrl": "/regression/2021/10/03/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D0.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "1. 머신러닝 개요",
            "content": "&#47672;&#49888;&#47084;&#45789;&#51060;&#46976;? . - Machine Learning은 컴퓨터가 데이터로부터 학습 하도록 컴퓨터를 프로그래밍하는 과학(또는 예술)입니다. 아래는 일반적인 정의입니다. . [Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed. -Arthur Samuel, 1959 . - 조금 더 공학적인 정의는 다음과 같습니다. . A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. -Tom Mitchell, 1997 . - 공학적인 정의를 잠깐 짚고 넘어가자면, 프로그램이 어떤 작업 $T$를 수행했을 때 그 성능 지표가 $P$이고 이 $P$가 어떤 경혐 $E$에 의해 향상되었을 때, 우리는 작업 $T$와 그 성능 지표 $P$에 관하여 &#39;프로그램이 경험 $E$를 통해 학습하였다&#39;고 말할 수 있다는 것입니다. . - Spam filter를 예로 들겠습니다. 스팸 필터는 일반 메일과 스팸 메일을 분류해주는 머신러닝 프로그램입니다. 이 프로그램은 저절로 뚝딱 만들어진 게 아니라 사용자(인간)에 의해 미리 분류된 수많은 분류 데이터들을 학습한 프로그램인 것이죠. 이 때 시스템이 학습하는 훈련 데이터들의 집단을 training set이라고 하고 개별 훈련 데이터 하나하나를 각각 training instance 또는 sample이라고 합니다. . . 참고문헌 . Aurelien Geron, &quot;Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems&quot;, O&#39;Reilly . | .",
            "url": "https://woohyeok-moon.github.io/Blog/machine%20learning/2021/10/02/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D1.html",
            "relUrl": "/machine%20learning/2021/10/02/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D1.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "11. 베이즈 정리",
            "content": "&#51204;&#54869;&#47456; &#44277;&#49885; . 사건 $B_1, B_2, dots, B_k$는 상호배반이며 $(B_i cap B_j = varnothing, i neq j, cup_{i=1}^k B_i=S)$라고 한다. 이 때 임의의 사건 A에 대하여 $$P(A)= sum_{i=1}^k P(B_i)P(A|B_i)$$ 가 성립한다. 이를 전확률 공식 또는 전체확률의 공식 이라고 합니다. . | 표본공간 $S$가 서로 배반인 사건 $B_1, B_2, dots, B_k$로 이루어져 있다면 그 표본공간 내부의 사건 $A$의 확률은 $A$와 $B_i$의 교집합들의 확률의 합으로 표현할 수 있습니다. $$P(A)=P(A cap B_i) + P(A cap B_2) + cdots + P(A cap B_k)$$ . 조건부 확률에서 배운 식을 적용하면 아래와 같은 식을 유도할 수 있습니다. $$P(A cap B_i) = P(B_i) cdot P(A|B_i)$$ $$P(A) = P(B_1)P(A|B_1) + P(B_2)P(A|B_2) + cdots + P(B_k)P(A|B_k)$$ . 베이즈 정리를 유도하기 위한 중간 과정이라고 생각해도 무방합니다. . | . . &#48288;&#51060;&#51592; &#51221;&#47532; . 사건 $B_1, B_2, dots, B_k$는 상호배반이며 $(B_i cap B_j = varnothing, i neq j, cup_{i=1}^k B_i=S)$라고 한다. 이 때 임의의 사건 A가 일어났다는 조건 하에서 사건 $B_j$가 일어날 확률은 $$P(B_j|A)= frac{P(B_j)P(A|B_j)}{ sum_{i=1}^k P(B_i)P(A|B_i)}$$ 가 성립한다. 이를 베이즈 정리 라고 합니다. . | 좌항을 변환해보면 $P(B_j|A)= frac{P(A cap B_j)}{P(A)}$이고, 분자에는 조건부 확률의 공식, 분모에는 전확률의 공식을 적용하면 아래의 식을 유도할 수 있습니다. $$ frac{P(B_j)P(A|B_j)}{ sum_{i=1}^k P(B_i)P(A|B_i)}$$ . 이는 추후 베이지안 통계학의 기초가 됩니다. . | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/09/25/%EC%88%98%ED%86%B511.html",
            "relUrl": "/statistical%20mathematics/2021/09/25/%EC%88%98%ED%86%B511.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "10. 확률의 곱셈정리",
            "content": "&#54869;&#47456;&#51032; &#44273;&#49480;&#51221;&#47532; . 조건부 확률에서 아래의 식을 배웠습니다. | . $$P(A|B) = frac{P(A cap B)}{P(B)}$$ . 이 식을 통해 다음의 관계식을 얻을 수 있습니다. | . $$P(A cap B) = P(B) cdot P(A|B)$$ $$P(A cap B) = P(A) cdot P(B|A)$$ . 이렇게 얻은 식을 우리는 확률의 곱셈정리 라고 합니다. 예를 들어 철수와 영희가 차례로 5개의 제비 중 2개의 당첨제비가 있는 제비뽑기를 한다고 가정했을 때 철수가 당첨되는 사건을 A, 영희가 당첨되는 확률을 B라고 하겠습니다. 철수가 뽑은 제비는 다시 넣지 않는 비복원추출입니다. 이 때 사건 A는 사건 B에 영향을 미치게 됩니다. 따라서 A와 B가 동시에 일어날 확률 $P(A cap B)$는 $P(A) cdot P(B|A)$가 됩니다. 직접 구해보면 아래와 같다는 것을 알 수 있을 것입니다. | . $$P(A cap B) = frac{2}{5} cdot frac{1}{4} = frac{1}{10}$$ . 하지만 만약 두 사건이 위와 다르게 서로 영향을 미치지 않는다면, 즉 독립이라면, $P(A|B) = P(A)$이므로 아래와 같은 식을 얻어낼 수 있습니다. | . $$P(A cap B) = P(A) cdot P(B)$$ .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/09/24/%EC%88%98%ED%86%B510.html",
            "relUrl": "/statistical%20mathematics/2021/09/24/%EC%88%98%ED%86%B510.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "9. 조건부 확률",
            "content": "저번 시간에 확률의 덧셈정리를 공부했습니다. 덧셈정리를 공부했으니 이제 곱셈정리를 공부할 차례입니다. 하지만 그 전에 알아야 할 것이 있는데, 바로 조건부 확률입니다. . &#51312;&#44148;&#48512; &#54869;&#47456; . - 어떠한 사건 B가 일어났을 때 사건 A가 일어났을 확률을 조건부 확률이라고 합니다. 기호로는 아래와 같이 나타냅니다. . $$P(A|B)= frac{P(A cap B)}{P(B)}$$ . 이해를 돕기 위해 일반적인 사건 A의 확률 $P(A)$ 의 관점에서 바라보겠습니다. 사건 A가 발생할 확률은 $P(A)$ 이며, 이는 $ frac{P(A)}{P(S)}$와 같다고 볼 수 있습니다. $P(S)=1$이기 때문에 생략되어 $P(A)$로 나타낼 수 있는 것이죠. . $P(A)$는 어떤 시행에서 A라는 사건이 일어날 확률을 구하는 것입니다. 이는 다르게 말하면 시행을 했다는 전제 하에 비로소 사건 A의 확률을 정의할 수 있다는 것입니다. 이 시행에서 일어날 수 있는 모든 사건들을 합한 것이 바로 표본공간입니다. 그렇다면 사건 A의 확률은 어떤 사건 S가 일어났을 때 사건 A의 확률로 정의될 수 있는 것이고, 이는 $P(A|S)= frac{P(A cap S)}{P(S)}= frac{P(A)}{1}=P(A)$ 로 표현될 수 있습니다. . | 이와 마찬가지로, 사건 B가 일어났을 때 사건 A가 일어났을 확률에서 분자는 $P(A cap B)$가 되고 분모는 $P(B)$ 가 됩니다. 사건 B가 일어났을 때 사건 A가 일어났다는 것은 사건 A와 B가 동시에 일어난다는 것을 뜻하며, 이 확률은 $P(A cap B)$ 로 구할 수 있습니다. 또한 이 때 사건 B는 &#39;조건부 확률이 아닌 일반적인 확률에서의 표본공간 S&#39;를 대신해주는 역할을 하기 때문에 분모로 들어가는 것입니다. . | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/20/%EC%88%98%ED%86%B59.html",
            "relUrl": "/statistical%20mathematics/2021/08/20/%EC%88%98%ED%86%B59.html",
            "date": " • Aug 20, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "8. 확률의 덧셈정리",
            "content": "&#54869;&#47456;&#51032; &#45927;&#49480;&#51221;&#47532; . - 서로 다른 사건 A와 B가 존재할 때, $A cup B$의 확률은 아래와 같습니다. . $$P(A cup B)=P(A)+P(B)-P(A cap B)$$ . . 서로 다른 집합 A와 B에서 아래와 같은 공식이 성립합니다. n(A)는 사건 A의 원소의 개수를 뜻합니다. . $$n(A cup B)=n(A)+n(B)-n(A cap B)$$ . 저희는 임의의 사건 A에 대하여 $P(A)= frac{n(A)}{n(S)}$ 임을 알고 있습니다. 따라서 모든 항에 n(S)를 나누어줍니다. . $$ frac{n(A cup B)}{n(S)}= frac{n(A)}{n(S)}+ frac{n(B)}{n(S)}- frac{n(A cap B)}{n(S)}$$ . $$P(A cup B)=P(A)+P(B)-P(A cap B)$$ . | . . &#50668;&#49324;&#44148;&#51032; &#54869;&#47456; . - 여사건 $A^c$의 확률은 아래와 같습니다. . $$P(A^c) = 1-P(A)$$ . 위 식은 아래와 같은 과정을 통해 유도할 수 있습니다. $$P(A) + P(A^c) = P(S)$$ $$P(S) = 1$$ $$P(A)+P(A^c)=1$$ $$P(A^c)=1-P(A)$$ | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/19/%EC%88%98%ED%86%B58.html",
            "relUrl": "/statistical%20mathematics/2021/08/19/%EC%88%98%ED%86%B58.html",
            "date": " • Aug 19, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "7. 확률 공리",
            "content": "&#54869;&#47456;&#51032; &#44277;&#47532;&#51201; &#51221;&#51032; . - 공리란 수학에서 정의를 하지 않기로 약속한 당연한 것으로 취급하는 명제를 말합니다. . 이러한 공리를 확률의 관점에서 바라보면, 확률을 말할 때 아주 당연하게 여겨지는 성질들을 모아 이러한 성질들을 만족해야만 확률이라고 정의할 수 있다고 합니다. 그리고 이러한 성질들을 바로 확률의 공리라고 합니다. 확률의 공리는 세 가지가 있습니다. | . $$P(S) = 1 quad (S ; is , a , sample , space)$$ $$if A subset S, quad 0 leq P(A) leq 1$$ $$if A_i cap A_j = varnothing ; (i neq j), quad cup_{i=1}^{ infty}A_i = sum_{i=1}^{ infty}P(A_i)$$ . &#39;확률&#39;로 표현되는 모든 것들은 위의 세 가지 성질을 만족합니다. | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/18/%EC%88%98%ED%86%B57.html",
            "relUrl": "/statistical%20mathematics/2021/08/18/%EC%88%98%ED%86%B57.html",
            "date": " • Aug 18, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "6. 순열과 조합(3)",
            "content": "&#51312;&#54633;(combination) . - 서로 다른 n개 중 r개를 순서에 관계없이 선택하는 것을 조합이라고 합니다. 기호로는 $_nC_r$ 로 나타내고, 계산식은 $ frac{n!}{r!(n-r)!}$ 입니다. . 지금까지 배운 순열과는 달리 조합에서는 순서를 고려하지 않기 때문에, 순서로 인해 발생하는 중복들을 제거해줘야 합니다. . | A부터 D까지의 알파벳 카드 중 3개의 카드를 뽑는 시행을 해보겠습니다. . 첫 번째 카드를 고를 때 가능한 경우는 A, B, C, D 총 4가지입니다. . 두 번째 카드를 고를 때 가능한 경우는 첫 번째 카드를 제외하여 3가지입니다. . 세 번째 카드를 고를 때 가능한 경우는 나머지 2가지입니다. . 따라서 가능한 경우의 수는 아래와 같습니다. $$4 * 3 * 2 = 24$$ . | 이 경우의 수는 순서를 고려한 경우의 수입니다. 하지만 저희는 순서와는 관계없는 단순한 뽑기를 진행하고 있기 때문에 순서로 인해 발생하는 중복들을 제거하는 작업이 필요합니다. 뽑은 카드가 A, B, C라고 가정했을 때, 이 세 카드에 의해 중복되는 경우를 구해보겠습니다. . (A, B, C) (A, C, B) (B, A, C) (B, C, A) (C, A, B) (C, B, A) . 총 6가지이며, $3!=3*2*1=6$ 과 같이 식으로도 계산할 수 있습니다. 저희는 중복되는 경우는 빼는 것이 아니라 나눠준다는 것을 배웠습니다. 여기서도 마찬가지로 6을 나눠주겠습니다. . $$ frac{24}{6}=4$$ . 따라서 경우의 수는 4이며, $_nC_r$을 사용하여 아래와 같이 구할 수 있습니다. $$_4C_3= frac{4!}{3!(4-3)!}= frac{24}{6}$$ . | . . &#51473;&#48373;&#51312;&#54633; . - 서로 다른 n개에서 r개를 중복을 허용하여 선택하는 것을 중복조합이라고 합니다. 기호로는 $_nH_r$로 나타내고, 계산식은 $_nH_r=_{n+k-1}C_r= frac{(n+k-1)!}{r!(n-1)!}$ 입니다. . A부터 D까지의 알파벳 중 하나를 중복을 허용하여 3번 선택한다고 가정해봅시다. 이 예제는 6개의 빈 칸 (□ □ □ □ □ □) 중 3개의 칸막이가 들어갈 칸을 선택하는 예제와 같습니다. 그리고 각 칸막이 사이에는 하나의 알파벳만 들어갈 수 있습니다. 뽑은 카드가 A, B, C라고 가정했을 때, 만약 1, 2, 3번째 칸을 선택했다면 | | | C C C 가 되는 것이고, 2, 4, 6번째 칸을 선택했다면 A | B | C | 가 되는 것이고, 1, 3, 5번째 칸을 선택했다면 | A | B | C 가 되는 것입니다. . | 이제 예제는 서로 다른 6개 중 4개를 중복을 허용하여 선택하는 경우로 바뀌었습니다. 이를 통해 $_nH_r=_{n+k-1}C_r$ 임을 알 수 있으며, 경우의 수는 아래와 같습니다. $$_4H_3=_6C_3= frac{6!}{3!3!}= frac{6*5*4}{3*2*1}=20$$ . | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/17/%EC%88%98%ED%86%B56.html",
            "relUrl": "/statistical%20mathematics/2021/08/17/%EC%88%98%ED%86%B56.html",
            "date": " • Aug 17, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "5. 순열과 조합(2)",
            "content": "&#50896;&#49692;&#50676; . - 서로 다른 n개에서 r개를 원형으로 배열하는 순열을 원순열이라고 합니다. 계산식은 $ frac{n!}{r(n-r)!}$ 입니다. . 숫자 1, 2, 3을 이용한 순열의 경우는 6가지가 존재합니다. . (1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1) . 이는 서로 다른 3개에서 3개를 배열하는 순열이기 때문에 아래와 같이 계산됩니다. . $$_3P_3=3!=3*2*1=6$$ . | 하지만 이 순열을 원형으로 배치한다면 결과가 다릅니다. . 위 그림과 같이 숫자를 원형으로 배열하게 되면, 회전하여 일치하는 경우는 모두 같은 경우이기 때문에 . 첫 번째, 네 번째, 다섯 번째 경우는 같은 경우입니다. . 두 번째, 세 번째, 여섯 번째 경우는 같은 경우입니다. . 때문에 경우의 수는 2로 줄어들게 됩니다. 이렇게 중복되는 경우는 빼주는 것이 아니라 나누어줘야 합니다. . 원순열에서는 회전하여 일치하는(중복되는) 경우의 수가 배열하는 원소의 개수(r)와 같기 때문에, r만큼 나누어줘야 합니다. 이를 식으로 나타내보겠습니다. . $$ frac{_nP_r}{r}= frac{n!}{r(n-r)!}$$ . | . . &#45796;&#44033;&#54805; &#49692;&#50676; . 저희는 원순열에서 회전하여 일치하는 경우를 같은 경우로 취급하였습니다. 그렇다면 원이 아닌 다각형에 배열한 순열의 경우는 어떻게 될까요? . - 이러한 순열을 다각형 순열이라고 부릅니다. . 마찬가지로 다각형 순열 또한 회전하여 일치하는 경우를 같은 경우로 취급합니다. . 첫 번째 그림(정사각형)의 경우, 회전시켰을 때 같은 모양이 되는 경우가 (처음 모양의 경우를 포함하여) 4개 존재합니다. 따라서 이를 식으로 나타내면 $ frac{n!}{4(n-r)!}$ 입니다. . 두 번째 그림(정삼각형)의 경우, 회전시켰을 때 같은 모양이 되는 경우가 3개 존재합니다. 따라서 이를 식으로 나타내면 $ frac{n!}{3(n-r)!}$ 입니다. . 세 번째 그림(직사각형)의 경우, 회전시켰을 때 같은 모양이 되는 경우가 2개 존재합니다. 따라서 이를 식으로 나타내면 $ frac{n!}{2(n-r)!}$ 입니다. . | . . &#46041;&#51088;&#49692;&#50676; . - 원순열, 다각형 순열 등과 같이 중복되는 경우가 존재하는 순열을 동자순열이라고 하고, 계산은 $_nP_r$에서 중복되는 경우의 수를 나눠줍니다. . 동자순열은 원순열과 다각형 순열 외에도 존재합니다. . 이와 같은 경우, 같은 색의 공끼리는 위치를 아무리 바꾸어도 동일한 경우가 됩니다. 따라서 빨간 색 공끼리 위치를 바꿀 경우의 수 $3!$만큼, 노란 색의 경우 $2!$만큼, 파란 색의 경우 $2!$만큼의 중복이 발생합니다. 중복을 고려하여 경우의 수를 계산해보겠습니다. $$ frac{_7P_7}{3!2!2!}= frac{7!}{24}=210$$ . | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/15/%EC%88%98%ED%86%B55.html",
            "relUrl": "/statistical%20mathematics/2021/08/15/%EC%88%98%ED%86%B55.html",
            "date": " • Aug 15, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "4. 순열과 조합(1)",
            "content": "&#49692;&#50676;(permutation) . - 서로 다른 n개에서 r개를 순서대로 나열하는 것을 순열이라고 합니다. 기호로는 $_nP_r$ 로 나타내고, 계산식은 n(n-1)(n-2)$ cdots$(n-r+1) 입니다. . 1부터 4까지 적힌 숫자 카드 4장 중 3장을 뽑아 숫자를 만들 때, 가능한 경우의 수를 구해봅시다. . 백의 자리에는 1, 2, 3, 4 총 4장의 카드 중 한 장을 선택할 수 있습니다. . 십의 자리에는 백의 자리에 들어간 한 장을 제외하고 남은 3장의 카드 중 한 장을 선택할 수 있습니다. . 일의 자리에는 남은 2장의 카드 중 한 장을 선택할 수 있습니다. . 위의 세 사건은 동시에 일어날 수 있기 때문에 곱의 법칙을 사용합니다. 따라서 가능한 경우의 수는 아래와 같습니다. $$4 * 3 * 2 = 24$$ . 기호로 표현하면, 서로 다른 4개에서 3개를 순서대로 나열하는 것이기 때문에 같이 표현됩니다. $$_4P_3 = 4(4-1)(4-2) = 4 * 3 * 2 = 24$$ . | . . &#44228;&#49849;(factorial) . - 자연수 n에 대하여 1부터 n까지 모든 자연수의 곱을 계승 또는 팩토리얼이라고 하고 기호로는 n$!$을 사용합니다. . 순열의 계산식을 보면 n(n-1)(n-2)$ cdots$ 와 같이 숫자가 내림차순으로 나열되어 곱해지는 구조를 가집니다. 이러한 식을 팩토리얼을 사용하여 간편하게 표현할 수 있습니다. $$_nP_r = frac{n!}{(n-r)!}$$ . 이 식을 사용하여 $_4P_3$을 표현해보겠습니다. $$_4P_3 = frac{4!}{(4-3)!} = frac{4 * 3 * 2 * 1}{1} = 24$$ . | . . &#51473;&#48373;&#49692;&#50676;(permutation with repetition) . - 서로 다른 n개에서 중복을 허용하여 r개를 순서대로 나열하는 것을 중복순열이라고 합니다. 기호로는 $_n Pi_r$ 로 나타내고, 계산식은$n^r$ 입니다. . 위의 시행에서 카드를 뽑고 난 뒤 카드를 돌려놓지 않고 남은 카드로 다음 자리의 숫자를 결정했습니다. 이처럼 이전의 추출로 인해 다음 추출에서 처음과는 다른 상태에서 경우의 수를 얻는 것을 비복원추출이라고 합니다. . | 반대로 숫자만 결정하고 카드를 빼가지 않는 경우에는 다음 자리 숫자를 결정할 때에도 선택지의 개수가 동일합니다. 이처럼 항상 처음과 같은 상태에서 경우의 수를 추출하는 것을 복원추출이라고 하고, 중복을 허용한다고 말합니다. 이 때 가능한 경우의 수를 구해봅시다. . 백의 자리에는 1, 2, 3, 4 총 4장의 카드 중 한 장을 선택할 수 있습니다. . 십의 자리 또한 1, 2, 3, 4 총 4장의 카드 중 한 장을 선택할 수 있습니다. . 일의 자리 또한 1, 2, 3, 4 총 4장의 카드 중 한 장을 선택할 수 있습니다. . 마찬가지로 곱의 법칙을 사용합니다. 따라서 가능한 경우의 수는 아래와 같습니다. . $$4 * 4 * 4 = 64$$ . 기호와 수식으로 표현할 때는 순열(P) 대신 중복순열($ Pi$)을 사용합니다. $$_4 Pi_3 = 4^3 = 64$$ . | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/14/%EC%88%98%ED%86%B54.html",
            "relUrl": "/statistical%20mathematics/2021/08/14/%EC%88%98%ED%86%B54.html",
            "date": " • Aug 14, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "3. 합의 법칙, 곱의 법칙",
            "content": "- 경우의 수를 구하는 방법에는 크게 두 가지가 있습니다. 바로 합의 법칙과 곱의 법칙 입니다. . &#54633;&#51032; &#48277;&#52825; . - 동시에 일어나지 않는 사건들의 경우의 수를 구할 때는 합의 법칙을 사용합니다. . 1에서 9까지의 숫자가 적힌 카드 9장 중 2장을 선택하는 시행을 할 때, 두 숫자의 합이 5 이하가 되는 사건 A의 경우의 수를 구해봅시다. . 합이 3인 경우의 수 : (1, 2) -&gt; 1개 . 합이 4인 경우의 수 : (1, 3) -&gt; 1개 . 합이 5인 경우의 수 : (1, 4), (2, 3) -&gt; 2개 . 합이 3인 경우와 합이 4인 경우는 동시에 일어날 수 없죠? 세 경우 모두 서로 동시에 일어날 수 없기 때문에 각각의 경우의 수를 합해줍니다. 따라서 사건 A의 경우의 수는 아래와 같습니다. $$1 + 1 + 2 = 4$$ . | 자연수 x, y로 이루어진 순서쌍에서 $x+y leq 4$ 인 사건 B의 경우의 수를 구해봅시다. . 합이 2인 경우의 수 : (1, 1) . 합이 3인 경우의 수 : (1, 2), (2, 1) . 합이 4인 경우의 수 : (2, 2) . 마찬가지로 사건 B의 경우의 수는 아래와 같습니다. $$1 + 2 + 1 = 4$$ . | 사건 A는 숫자의 중복 사용이 혀용되고, 사건 B는 숫자의 중복 사용이 허용되지 않는다는 차이가 있습니다. 그러나 그것과 별개로 나뉜 사건들이 동시에 일어날 수 없기 떄문에 두 사건 모두 합의 법칙을 사용합니다. . | . . &#44273;&#51032; &#48277;&#52825; . - 동시에 일어나는 두 사건의 경우의 수를 구할 때는 곱의 법칙을 사용합니다. . 주사위와 동전을 동시에 던지는 사건 C의 경우의 수를 구해봅시다. . 주사위를 통해 나올 수 있는 경우의 수 : 6 동전을 통해 나올 수 있는 경우의 수 : 2 . 합의 법칙 때와는 달리 주사위와 동전은 동시에 일어날 수 있는 사건입니다. 따라서 사건 C의 경우의 수는 $6 * 2 = 12$ 로 12개가 됩니다. . | 주사위 1개를 2번 던지는 사건 D의 경우의 수를 구해봅시다. . 주사위를 통해 나올 수 있는 경우의 수 : 6 주사위를 통해 나올 수 있는 경우의 수 : 6 . 마찬가지로 사건 D의 경우의 수는 $6 * 6 = 36$ 으로 36개가 됩니다. . | . . &#51032;&#47928;&#51216; . - 여기서 한 가지 의문이 발생합니다. . 사건 D의 경우 1개의 주사위를 두 번 던진다고 하였습니다. 여러분은 1개의 주사위를 동시에 두 번 던질 수 있나요? . 제가 해석한 &#39;동시&#39;의 개념은, 한 번의 시행에 각각의 사건이 함께 발생할 수 있는가? 입니다. . 사건 A로 돌아가봅시다. 합이 3인 사건 (1, 2)와 합이 4인 사건 (1, 3) 이 한 번의 시행에 함께 발생할 수 있나요? 아닙니다. 합이 3인 사건이 발생하는 순간 시행이 종료되기 때문에, 같은 시행에서 합이 4인 사건은 절대 일어날 수 없습니다. . 이번엔 사건 D로 가봅시다. 첫 번째 주사위의 눈이 나오는 사건과 두 번째 주사위의 눈이 나오는 사건이 한 번의 시행에 함께 발생할 수 있나요? 그렇습니다. 시행 자체가 주사위를 2번 던지는 것이기 때문에, 첫 번째 주사위를 던진다고 해도 시행이 종료되지 않습니다. 이후 두 번째 주사위를 던져야만 비로소 한 번의 시행이 끝나겠죠. . | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/13/%EC%88%98%ED%86%B53.html",
            "relUrl": "/statistical%20mathematics/2021/08/13/%EC%88%98%ED%86%B53.html",
            "date": " • Aug 13, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "2. 확률의 개념(2)",
            "content": "&#49324;&#44148;&#51032; &#51333;&#47448; . 표본공간의 어떤 부분집합이냐에 따라 사건의 종류가 나뉩니다. . 임의의 표본공간 S와 사건 A, B가 존재한다고 가정하겠습니다. . 합사건 $A cup B$ : A 또는 B 중 적어도 하나에는 일어나는 사건 | 곱사건 $A cap B$ : A 그리고 B에 모두 일어나는 사건 | 여사건 $A^ mathsf{c}$ : A의 원소들을 제외한 표본공간 S의 모든 원소들의 집합인 사건 | 배반사건 $A cap B = varnothing$ : 임의의 사건 A와 B에 공통원소가 하나도 존재하지 않을 때 배반사건이라고 합니다. | 공사건 $ varnothing$ : 표본공간의 원소를 하나도 포함하지 않는 사건 (원소의 개수가 0이다.) | 전사건 : 표본공간 S의 원소를 모두 포함하는 사건 | . . &#54869;&#47456;&#51032; &#51333;&#47448; . - 확률은 크게 네 가지 종류가 존재합니다. 수학적 확률, 기하학적 확률, 통계적 확률, 주관적 확률입니다. . . &#49688;&#54617;&#51201; &#54869;&#47456; . - 근원사건이 일어날 가능성이 모두 같을 때, 확률을 구하고 싶은 사건 A의 원소의 개수를 표본공간 S의 원소의 개수로 나눈 값을 수학적 확률이라고 합니다. . 동전을 던지는 시행에서, 앞이 나오는 사건을 A라고 가정하겠습니다. 이 때 표본공간 S와 사건 A는 아래와 같이 정의할 수 있습니다. $$S = {앞앞, 앞뒤, 뒤앞, 뒤뒤 }$$ $$A = {앞앞, 앞뒤, 뒤앞 }$$ . | 그렇다면 사건 A가 일어날 수학적 확률을 구해볼까요? $$P(A) = frac{n(A)}{n(S)} = frac{3}{4}$$ $$(n(A)=사건 A의 원소의 개수)$$ . | 한 가지 더 살펴보겠습니다. 저희는 사건이 표본공간의 부분집합임을 배웠습니다. 사건이 될 수 있는 집합들을 나열해보겠습니다. . $$ varnothing, {앞앞 }, cdots , {$앞뒤, 뒤앞, 뒤뒤$ }, S$$ . 보아하니, $P( varnothing)=0, P(S)=1$ 이 되고, 그 사이에 존재하는 사건의 확률들은 공사건과 전사건의 확률의 사이값임을 알 수 있군요. 여기서 알 수 있는 사실이 한 가지 있습니다. . 확률은 0 ~ 1 사이의 값을 가지게 된다는 것이죠! . | . . &#44592;&#54616;&#54617;&#51201; &#54869;&#47456; . - 표본공간 S와 사건 A를 임의의 면적이라고 생각하였을 때, A가 차지하는 면적을 S가 차지하는 면적으로 나눈 값을 기하학적 확률이라고 합니다. . 앞서 수학적 확률에서는 원소의 개수를 세어 확률을 구했습니다. . 책상 위에 색종이가 한 장 존재합니다. 그 색종이는 &#39;면적&#39;을 가지고 있겠죠. 여러분은 그 색종이 안에 있는 점들을 셀 수 있나요? 이처럼 원소를 셀 수 없는 기하학적 상황에서는 선의 길이, 면의 넓이, 도형의 부피 등으로 확률을 나타냅니다. . | . . &#53685;&#44228;&#51201; &#54869;&#47456; . - 실제로 무수히 많은 시행을 관측하여 얻어낸 횟수를 통해 정의하는 확률을 통계적 확률 이라고 합니다. . 주사위를 던졌을 때 1이 나올 확률을 수학적 확률에서는 $ frac{1}{6}$으로 바로 결론을 도출합니다. 하지만 실제로 이 주사위를 던졌을 때 $ frac{1}{6}$의 확률로 1이 나올 것인지는 누구도 보장할 수 없습니다. 그렇기 때문에 이 시행을 직접 해보는 것입니다. | . 이 떄 그냥 몇 번 던져보고 나오는 값만으로 확률을 알아볼 수는 없습니다. 10번 던졌을 때보다, 100번 던졌을 때, 1,000번, 10,000번, ... 이렇게 시행의 횟수가 커질수록 1이 나올 확률은 $ frac{1}{6}$에 수렴하게 됩니다. 이러한 것을 큰 수의 법칙이라고 합니다. 무수히 많은 시행을 관측하고, 그렇게 나온 결과를 반영했을 때 비로소 통계적 확률을 구한다고 말할 수 있습니다. | . . &#51452;&#44288;&#51201; &#54869;&#47456; . - 지금까지의 전통적 확률과는 달리, 주관적인 믿음의 정도를 0 ~ 1 사이의 숫자로 나타낸 것을 주관적 확률이라고 합니다. . 이를테면 주사위를 던졌을 때 1, 2, ..., 6이 나올 확률이 각각 $ frac{1}{6}$이라는 주장은, &#39;각 숫자가 나올 가능성은 동일하다&#39;라는 믿음을 반영한 것입니다. 이 개념의 도입으로 많은 것들이 가능해졌습니다. 이를테면, 지금까지 시행의 조건으로 &#39;동일한 조건에서 반복 가능할 것&#39;을 강요받아왔습니다. 그렇기에 일회성 사건이나 비용이 많이 필요한 연구가설 등은 확률실험의 대상이 될 수 없었습니다. 반면 주관적 확률은 시행의 조건에서 자유롭기 때문에 이러한 것들의 확률을 논할 수 있습니다. 이는 추후 베이즈 정리를 공부할 때 필요한 개념이며, 더 자세한 것은 그 때 다루도록 하겠습니다. | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/12/%EC%88%98%ED%86%B52.html",
            "relUrl": "/statistical%20mathematics/2021/08/12/%EC%88%98%ED%86%B52.html",
            "date": " • Aug 12, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "1. 확률의 개념(1)",
            "content": ". &#49436;&#47200; . 머신러닝, 딥러닝 공부를 하다보니 통계학에 대해 한 번 정리하고 넘어가야겠다는 생각을 하게 되었습니다. . 여러분은 고등학교 시절부터 지금까지, 확률과 통계라는 단어를 종종 접해보셨을 겁니다. 확률과 통계는 엄연히 다른 개념입니다. 통계를 알지 못해도 확률을 배울 수는 있지만, 확률을 알지 못하면 통계를 배울 수 없습니다. . . &#49324;&#51204; &#51648;&#49885; . - 확률을 공부하기 전에 알아두어야 할 사전 지식이 있습니다. 바로 시행, 표본공간, 사건 입니다. . . &#49884;&#54665;(trial) . - 시행이란 동일한 조건에서 여러 번 반복할 수 있고, 그 결과가 우연에 의해 결정되는 &#39;관찰 또는 실험&#39;을 말합니다. . 확률을 구하는 실험으로 주사위 실험과 동전 던지기 실험이 빈번하게 등장하곤 합니다. 이는 두 실험이 저 조건을 모두 만족하는 시행에 해당되기 때문입니다. . | A가 주사위 또는 동전을 6번 던지는 실험을 가정해보겠습니다. 이 때, A의 첫 번째 시행과 두 번째 시행, n 번째 시행의 조건은 다르지 않습니다. 한 번 던졌다고 해서 주사위나 동전이 사라지거나 손상되지는 않기 때문에 반복 가능합니다. 또한 저희가 이 실험의 결과를 의도적으로 바꿀 수 없기 때문에 우연에 의해 결과가 결정된다고 할 수 있습니다. . | . - 시행의 조건을 정하는 데는 이유가 있습니다. . 저희는 통계적 확률을 구하려고 합니다. 허나 동일한 조건에서 여러 번 반복할 수 없다면, 시행의 횟수가 적어 유의미한 확률을 구할 수 없을 것입니다. 또한 결과가 우연에 의해 일어나는 것이 아니라 어떤 원인에 의해 필연적으로 일어난다면, 확률을 구하는 것 보다는 그 원인과 결과의 관계성을 추론하는 것이 더 바람직한 행위일 것입니다. | . . &#54364;&#48376;&#44277;&#44036;(sample space) . - 어떠한 시행을 통해 얻을 수 있는 모든 결과들의 집합을 바로 표본공간이라고 합니다. 기호는 S로 표기하겠습니다. . 동전을 두 번 던져 앞면 또는 뒷면을 확인하는 시행을 가정하겠습니다. 표본공간은 아래와 같이 표현할 수 있습니다. . $$S= {앞앞, 앞뒤, 뒤앞, 뒤뒤 }$$ . 이 시행에서 저 표본공간 밖의 결과는 얻어낼 수 없습니다. . | . . &#49324;&#44148;(event) . - 사건은 표본공간의 부분집합을 의미합니다. . 예를 들어, 위의 실험에서 &#39;앞면이 1번 이상 나오는 사건 A&#39;와 &#39;뒷면이 나오지 않는 사건 B&#39;는 아래와 같습니다. $$A= {앞앞, 앞뒤, 뒤앞 }$$ $$B= {앞앞 }$$ . | 조금 더 생각해보면, 사건은 절대 표본공간을 벗어나지 못함을 알 수 있습니다. 표본공간 부분 마지막 문장과 연결하여 생각해보는 것을 추천합니다. . | . . &#44540;&#50896;&#49324;&#44148;(fundamental event) . - 추가적으로, 표본공간에서 원소의 개수가 하나인 사건을 근원사건이라고 합니다. . 이 실험에서 근원사건은 $ {$앞앞$ }$, $ {$앞뒤$ }$, $ {$뒤앞$ }$, $ {$뒤뒤$ }$가 되겠네요. 위의 사건 B 또한 근원사건이라고 볼 수 있겠죠. | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/11/%EC%88%98%ED%86%B51.html",
            "relUrl": "/statistical%20mathematics/2021/08/11/%EC%88%98%ED%86%B51.html",
            "date": " • Aug 11, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "0. 수리통계학 정리",
            "content": "1. &#54869;&#47456;&#51060;&#47200; . 1.1 . 두 개의 사건 A와 B에 대하여 다음과 같은 성질들이 성립한다. . $$P(A^c)=1-P(A)$$ $$P( varnothing)=0$$ $$A subset B이면 P(A) leq P(B)이다.$$ $$P(A cup B) = P(A) + P(B) - P(A cup B)$$ . . 1.2 (&#51204;&#54869;&#47456;&#44277;&#49885;) . 사건 $B_1, B_2, cdots , B_k$는 상호배반이며 $(B_i cap B_j = varnothing, i neq j), cup_{i=1}^{k}B_i=S$라고 하자. 이 때 임의의 사건 A에 대하여, . $$P(A)= sum_{i=1}^{k}P(B_i)P(A|B_i)$$ . 가 성립한다. . . 1.3 (&#48288;&#51060;&#51592; &#51221;&#47532;) . 사건 $B_1, B_2, cdots , B_k$는 상호배반이며 $(B_i cap B_j = varnothing, i neq j), cup_{i=1}^{k}B_i=S$라고 하자. 이 때 사건 A가 일어났다는 조건하에서 사건 B_j가 일어날 확률은 . $$P(B_j|A)= frac{P(B_j)P(A|B)}{ sum limits_{i=1}^{k}P(B_i)P(A|B_i)}$$ . 로 주어진다. . . 2. &#54869;&#47456;&#48320;&#49688; . 2.1 . 함수 $F(x)$가 어떤 확률변수 $X$의 누적분포함수가 되는 필요충분조건은 다음과 같다. . $$ lim_{x to - infty} F(x) = 0$$ $$ lim_{x to infty} F(x) = 1$$ $$ lim_{h to 0^+} F(x+h) = F(x)$$ $$a&lt;b이면 ,F(a) leq F(b)$$ . . 2.2 . 연속형확률변수 $X$의 확률밀도함수가 $f(x)$이고 확률분포함수가 $F(x)$이면, $f(x)= frac{d}{dx} F(x)$를 만족한다. . . 2.3 . 함수 $F(x1, x2)$가 어떤 이변량 확률벡터 $(X_1, X_2)$의 결합 확률분포함수가 되는 필요충분조건은 다음과 같다. . 모든 $x_1$에 대하여 $ lim_{x_2 to - infty} F(x_1, x_2) = F(x_1, - infty) = 0$ 모든 $x_2$에 대하여 $ lim_{x_1 to - infty} F(x_1, x_2) = F(- infty, x_2) = 0$, $ lim_{x1 to infty, x2 to infty} F(x_1, x_2) = F( infty, infty) = 1$ 모든 $a&lt;b$, $c&lt;d$에 대하여 $F(b, d) - F(b, c) - F(a, d) - F(a, d) + F(a, c) geq 0$ 모든 $x_1, x_2$에 대하여 $ lim_{h to 0^+} F(x_1, x_2 + h) = lim_{h to 0^+} F(x_1 + h, x_2) = F(x_1, x_2)$ . . 2.4 . 두 확률변수 $X, Y$의 결합 확률밀도함수가 $f_{X, Y}(x, y)$로 주어졌을 때, $X$와 $Y$ 각각의 주변 확률밀도함수 $f_X(x)$와 $f_Y(y)$는 . (1) 이산형인 경우 $$f_X(x) = sum_{ forall{y}}f_{X,Y}(x,y), f_Y(y)= sum_{ forall{x}}f_{X,Y}(x,y)$$ . (2) 연속형인 경우 $$f_X(x) = int_{- infty}^{ infty}f_{X,Y}(x,y) mathrm{d}y, ; f_Y(y) = int_{- infty}^{ infty}f_(X,Y)(x,y) mathrm{d}x$$ 로 구해진다. . . 2.5 . 두 확률변수 $X$와 $Y$가 서로 독립일 필요충분조건은 . $$f_{X,Y}(x,y) = f_X(x) cdot f_Y(y)$$ . 이다. . . 2.6 . 확률변수 $X$의 함수에 대한 기댓값은, 상수 $a, b, c$에 대하여 . $$E(c)=c$$ $$E(aX+b)=aE(X)+b$$ . 를 만족한다. . . 2.7 . 두 확률변수 $X$와 $Y$가 서로 독립이면 . $$E(XY)=E(X) cdot E(Y)$$ . 가 성립한다. 또한, $X$와 $Y$의 함수인 $g(X)$와 $h(Y)$도 독립이며 . $$E[g(X)h(Y)] = E[g(X)] cdot E[h(Y)]$$ . 이다. . . 2.8 . (1) 확률변수 $X, Y$에 대하여, $Y=aX+b$이면 . $$Var(y)=a^2Var(X)$$ . (2) 확률변수 $X_1,X_2, cdots,X_n$이 서로 독립이면 . $$Var( sum_{i=1}^n X_i)= sum_{i=1}^n Var(X_i)$$ . . 2.9 . 두 확률변수 $X,Y$의 공분산은 . $$ begin{aligned} Cov(X,Y)=&amp;E[(X-EX)(Y-EY)] =&amp;E(XY)-E(X)E(Y) end{aligned}$$ . 2.10 . 확률변수 $X_1, X_2, cdots, X_n$이 있을 때, . $$Var( sum_{i=1}^n X_i) = sum_{i=1}^n Var(X_i) + 2 sum_{j&lt;k}Cov(X_j, X_k)$$ . 가 성립한다. . . 2.11 . 두 확률변수 $X, Y$에 대하여 . $$E[E(Y|X)] = E(Y)$$ . 가 성립한다. . . 2.12 . 확률변수 $X$와 $Y$가 독립이면 . $$E(Y|x) = E(Y), E(X|y) = E(X)$$ . 가 성립한다. . . 2.13 . 두 확률변수 $X, Y$에 대하여 . $$Var(Y) = E[Var(Y|X)] + Var[E(Y|X)]$$ . 가 성립한다. . . 2.14 &#47560;&#53076;&#54532; &#54869;&#47456;&#48512;&#46321;&#49885; . 실함수 $u(x) &gt; 0$라고 할때, 확률변수 $X$는 임의의 상수 $c&gt;0$에 대하여 . $$P[u(X) geq c] leq frac{E[u(X)]}{c}$$ . 를 만족한다. . . 2.15 &#52404;&#48708;&#49520;&#54532; &#48512;&#46321;&#49885; . 확률변수 $X$의 평균이 $ mu$이고 분산이 $ sigma^2 &lt; infty$이면, 임의의 $k&gt;0$에 대하여 . $$P[|X- mu| geq k sigma] leq frac{1}{k^2}$$ . 이 성립한다. . . 2.16 &#53076;&#49884;-&#49800;&#48148;&#47476;&#52768; &#48512;&#46321;&#49885; . 두 확률변수 $X, Y$에 대해서 $E(X^2)&lt; infty, E(Y^2)&lt; infty$가 만족된다면, . $$[E(XY)]^2 leq E(X^2) cdot E(Y^2)$$ . 이 성립한다. (단, 등식은 어떤 상수 $c$에 대하여 $P(Y-cX)=1$인 경우에 성립함.) . . 2.17 . 확률변수 $X$가 $B(n, p)$ 분포를 따른다고 하자. 이때 $n$이 커짐에 따라 $p$가 0으로 수렴하되 $np= lambda$를 만족하면, $x=0, 1, 2, cdots$에 대해서 . $$ lim_{n to infty} begin{pmatrix} n x end{pmatrix} p^x(1-p)^(n-x) = frac{exp(- lambda) lambda^x}{x!}$$ . 이 성립한다. . . 2.18 . $X sim GEO(p)$이면, 임의의 자연수 $j, k$에 대하여, . $$P(X&gt;j+k|X&gt;j)=P(X&gt;k)$$ . 가 성립한다. . . 참고문헌 . &quot;수리통계학&quot; by 송성주, 전명식. -자유아카데미- . | .",
            "url": "https://woohyeok-moon.github.io/Blog/statistical%20mathematics/2021/08/10/%EC%88%98%ED%86%B50.html",
            "relUrl": "/statistical%20mathematics/2021/08/10/%EC%88%98%ED%86%B50.html",
            "date": " • Aug 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "통계학과 2학년 학부생 . | AI 대학원 준비 중 . | .",
          "url": "https://woohyeok-moon.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://woohyeok-moon.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}